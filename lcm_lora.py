# -*- coding: utf-8 -*-
"""LCM LoRA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HHLFQvsSrmlbl0EGJzZD61Y-EinjOmsA
"""

import torch

!pip install --upgrade diffusers[torch]

from diffusers import DiffusionPipeline, LCMScheduler

model_id = "stabilityai/stable-diffusion-xl-base-1.0"
lcm_lora_id = "latent-consistency/lcm-lora-sdxl"

pipe = DiffusionPipeline.from_pretrained(model_id, variant = "fp16")

pipe.load_lora_weights(lcm_lora_id)

pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

pipe.to(device="cuda", dtype=torch.float16)

prompt = "close-up photography of an old man standing in the rain at night, in a street light by lamps, leica 35mm summilux"

images = pipe(
    prompt = prompt,
    num_inference_steps = 5,
    guidance_scale = 1,
).images[0]

images

images = []
for steps in range(8):
  generator = torch.Generator(device = pipe.device).manual_seed(1337)
  image = pipe(
      prompt = prompt,
      num_inference_steps = steps+1,
      guidance_scale = 1,
      generator = generator,
  ).images[0]
  images.append(image)

images

images[7]



"""# Few Info

1. Guidance scale: 0 to 2

2. If 0 to 1: negative prompts are ignored, and if it is 1-2, negative prompts and bit slower
"""

pipe.unload_lora_weights()

from diffusers import EulerDiscreteScheduler

pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)

images = []
for steps in (1, 4, 8, 15, 25, 30, 50):
  generator = torch.Generator(device = pipe.device).manual_seed(1337)
  image = pipe(
      prompt = prompt,
      num_inference_steps = steps,
      generator = generator,
  ).images[0]
  images.append(image)

images

images[6]

"""## LCM LoRA collection

1. lcm lora sdxl

2. sdv1.5

3. lcm lora ssd 1b
"""

